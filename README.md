# 🤖 GPT-2 Text Generation with Decoding Strategies

This project demonstrates how to use the pre-trained GPT-2 model for text generation using various decoding techniques and prompt styles.

## 📌 Key Topics
- Tokenization using HuggingFace tokenizer
- Greedy, Sampling, Top-k, Top-p decoding
- Temperature control
- Prompt engineering (instruction, dialogue, storytelling, Q&A)
- Output length control (`min_length`, `max_length`)

## 📁 Files
- `gpt2_generation.ipynb`: Main notebook with step-by-step explanations and code

## 🚀 Getting Started
1. Open in Google Colab or Jupyter
2. Install dependencies:
   ```bash
   pip install transformers torch
Run through each section to explore GPT-2 behavior

🙌 Acknowledgments
Built as part of a foundational LLM learning roadmap, focusing on intuition + implementation.








